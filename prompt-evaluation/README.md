# Prompt Evaluation

A minimal, **rubric-driven** evaluation workflow for LLM responses using Python and Pandas.
The goal is to demonstrate a repeatable approach to analyzing **accuracy, clarity, tone**, and **instructional alignment**.

## Files
- `prompt_evaluation.py` — Example evaluation workflow (run locally: `python prompt_evaluation.py`).
- `sample_prompts.csv` — Example K–12 prompts.
- `rubric.json` — Simple, editable rubric for scoring dimensions.

> Note: The example uses **simulated responses**. In production, replace the placeholders with API responses and store results in a database or dashboard tool.
